{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "with open(\"config.json\", 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "AZURE_OPENAI_API_KEY = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT = config[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "DEPLOYMENT_NAME = config[\"DEPLOYMENT_NAME\"]\n",
    "# Azure OpenAI details\n",
    "\n",
    "# Create OpenAI client\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "# Call the GPT-4o model\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT_NAME,  # Deployment name of GPT-4o\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/9ybby8756ql0sh417vwrbf_58j7sdg/T/ipykernel_26278/3291561183.py:3: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm1 = AzureChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm1 = AzureChatOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY, \n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "    deployment_name=DEPLOYMENT_NAME, \n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-02-01\")\n",
    "\n",
    "response = llm1.invoke(\"What is the capital of France?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm1\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHere is our conversation:\n",
      "\n",
      " User: \n",
      "What is the capital of France?\n",
      " AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of France is Paris.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHere is our conversation:\n",
      "Human: What is the capital of France?\n",
      "AI: The capital of France is Paris.\n",
      " User: \n",
      "What is its population?\n",
      " AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of the most recent data, the population of Paris is approximately 2.1 million people. However, the population of the larger Paris metropolitan area, known as Île-de-France, is around 12 million people. Population figures can change, so it's always a good idea to check the latest statistics from a reliable source.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHere is our conversation:\n",
      "Human: What is the capital of France?\n",
      "AI: The capital of France is Paris.\n",
      "Human: What is its population?\n",
      "AI: As of the most recent data, the population of Paris is approximately 2.1 million people. However, the population of the larger Paris metropolitan area, known as Île-de-France, is around 12 million people. Population figures can change, so it's always a good idea to check the latest statistics from a reliable source.\n",
      " User: \n",
      "what is the ratio of men vs woman?\n",
      " AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of the most recent data, the population ratio of men to women in Paris is roughly 47% men to 53% women. This ratio is generally consistent with broader trends in many urban areas, but it can vary slightly depending on the specific dataset and the year of the data collection. For the most precise and up-to-date information, consulting the latest statistics from official sources such as the French National Institute of Statistics and Economic Studies (INSEE) would be recommended.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"question\"], template=\"Here is our conversation:\\n{history}\\n User: \\n{question}\\n AI:\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm1,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True)\n",
    "\n",
    "print(llm_chain.invoke({\"question\": \"What is the capital of France?\"})[\"text\"])\n",
    "print(llm_chain.invoke({\"question\": \"What is its population?\"})[\"text\"])\n",
    "print(llm_chain.invoke({\"question\": \"what is the ratio of men vs woman?\"})[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
